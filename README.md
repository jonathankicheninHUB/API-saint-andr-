# ğŸ—³ï¸ OODA Pipeline - Saint-AndrÃ© (1976-2026)

![Status](https://img.shields.io/badge/Status-Live-success)
![Frontend](https://img.shields.io/badge/Frontend-Vercel-black)
![Backend](https://img.shields.io/badge/Backend-Render-purple)
![Data](https://img.shields.io/badge/Data-Google%20Drive-blue)

Une application Full-Stack de **Data Intelligence Ã‰lectorale** pour la commune de Saint-AndrÃ© (La RÃ©union). Ce projet automatise la collecte, le traitement et la visualisation des donnÃ©es Ã©lectorales et de presse sur 50 ans.

ğŸ”— **AccÃ¨s au Tableau de Bord :** [Lien de votre site Vercel ici]
ğŸ”— **AccÃ¨s Ã  l'API :** [https://api-saint-andr.onrender.com](https://api-saint-andr.onrender.com)

---

## ğŸ—ï¸ Architecture Technique

Le projet est conÃ§u en **Monorepo** (Backend et Frontend dans le mÃªme dÃ©pÃ´t) et dÃ©ployÃ© sur une architecture Cloud sans serveur (Serverless).

```mermaid
graph LR
    A[Scraper Python] -->|Write JSON| B(Google Drive)
    C[API FastAPI] -->|Read JSON| B
    D[Frontend React] -->|Fetch Data| C
    E[Utilisateur] -->|View| D
````

| Composant | Technologie | HÃ©bergement | RÃ´le |
| :--- | :--- | :--- | :--- |
| **Backend** | Python, FastAPI, Scrapy | **Render** | API REST & Moteur de Scraping |
| **Frontend** | React.js (Create React App) | **Vercel** | Interface Utilisateur & Dashboard |
| **Database** | JSON Flat File | **Google Drive** | Stockage persistant des donnÃ©es |

-----

## ğŸ“‚ Structure du Projet

```bash
API-saint-andr-/
â”œâ”€â”€ scraper_backend/        # ğŸ LE MOTEUR (Python)
â”‚   â”œâ”€â”€ api/                # Code de l'API FastAPI
â”‚   â”‚   â””â”€â”€ main.py         # Points d'entrÃ©e (Endpoints)
â”‚   â”œâ”€â”€ scraper/            # Le Robot Scrapy
â”‚   â”‚   â”œâ”€â”€ spiders/        # Logique de collecte
â”‚   â”‚   â”œâ”€â”€ pipelines.py    # Export vers Drive
â”‚   â”‚   â””â”€â”€ items.py        # SchÃ©ma des donnÃ©es
â”‚   â””â”€â”€ requirements.txt    # DÃ©pendances Python
â”‚
â””â”€â”€ web_frontend/           # âš›ï¸ L'INTERFACE (React)
    â”œâ”€â”€ public/             # Fichiers statiques
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ views/          # Pages (Dashboard)
    â”‚   â”œâ”€â”€ services/       # Connexion API
    â”‚   â””â”€â”€ ...
    â””â”€â”€ package.json        # DÃ©pendances Node.js
```

-----

## ğŸš€ FonctionnalitÃ©s ClÃ©s

### 1\. Scraping AutomatisÃ© (`/trigger-scrape`)

Un robot Scrapy intelligent visite les sources de donnÃ©es ciblÃ©es, extrait les informations clÃ©s (RÃ©sultats Ã©lectoraux, Population, Presse) et gÃ©nÃ¨re un rapport structurÃ©.

### 2\. Monitoring IntÃ©grÃ©

Le systÃ¨me surveille sa propre santÃ©. Chaque exÃ©cution du robot est journalisÃ©e (Logs, DurÃ©e, Statut) et visible directement sur le Dashboard.

### 3\. Stockage Cloud SÃ©curisÃ©

Aucune base de donnÃ©es complexe. Les donnÃ©es sont stockÃ©es sous forme de `Master JSON` sur un Google Drive sÃ©curisÃ©, accessible via un Compte de Service Google Cloud.

-----

## ğŸ› ï¸ Installation & DÃ©ploiement

### PrÃ©-requis

  * Un compte **GitHub**
  * Un compte **Render** (pour le Backend)
  * Un compte **Vercel** (pour le Frontend)
  * Un **Service Account Google Cloud** (Fichier JSON)

### 1\. DÃ©ploiement Backend (Render)

1.  CrÃ©er un **Web Service** sur Render connectÃ© Ã  ce dÃ©pÃ´t.
2.  **Runtime :** Python 3
3.  **Root Directory :** (Laisser vide)
4.  **Build Command :** `pip install -r scraper_backend/requirements.txt`
5.  **Start Command :** `uvicorn scraper_backend.api.main:app --host 0.0.0.0 --port $PORT`
6.  **Variables d'Environnement :**
      * `GOOGLE_DRIVE_MASTER_FOLDER_ID` : ID du dossier Drive
      * `SERVICE_ACCOUNT_JSON` : Contenu du fichier clÃ© Google
      * `GOOGLE_DRIVE_CREDENTIALS_PATH` : `./service_account_key.json`

### 2\. DÃ©ploiement Frontend (Vercel)

1.  Importer le projet sur Vercel.
2.  **Framework Preset :** Create React App
3.  **Root Directory :** `web_frontend`
4.  **Build Command :** `npm run build`
5.  **Output Directory :** `build`

-----

## ğŸ® Utilisation de l'API

| MÃ©thode | Endpoint | Description |
| :--- | :--- | :--- |
| `GET` | `/` | VÃ©rifier le statut de l'API |
| `GET` | `/health` | Healthcheck complet (Drive, API) |
| `GET` | `/kpis` | RÃ©cupÃ©rer les donnÃ©es pour le Dashboard |
| `GET` | `/trigger-scrape` | **Action :** Lancer le robot de scraping manuellement |

-----

## ğŸ›¡ï¸ SÃ©curitÃ©

  * Les clÃ©s d'API ne sont **jamais** stockÃ©es dans le code (utilisation des variables d'environnement).
  * L'authentification Google Drive utilise un fichier temporaire gÃ©nÃ©rÃ© Ã  la volÃ©e.

-----

**Â© 2025 - OODA Pipeline Project**

```
```
